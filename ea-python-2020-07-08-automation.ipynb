{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"earth-lab-logo-rgb.png\" width=\"150\" height=\"150\" />\n",
    "\n",
    "# Homework Template: Earth Analytics Python Course: Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting this assignment, be sure to restart the kernel and run all cells. To do this, pull down the Kernel drop down at the top of this notebook. Then select **restart and run all**.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below.\n",
    "\n",
    "* IMPORTANT: Before you submit your notebook, restart the kernel and run all! Your first cell in the notebook should be `[1]` and all cells should run in order! You will lose points if your notebook does not run. \n",
    "\n",
    "For all plots and code in general:\n",
    "\n",
    "* Add appropriate titles to your plot that clearly and concisely describe what the plot shows (e.g. time, location, phenomenon).\n",
    "* Be sure to use the correct bands for each plot.\n",
    "* Specify the source of the data for each plot using a plot caption created with `ax.text()`.\n",
    "* Place ONLY the code needed to create a plot in the plot cells. Place additional processing code ABOVE that cell (in a separate code cell).\n",
    "\n",
    "Make sure that you:\n",
    "\n",
    "* **Only include the package imports, code, data, and outputs that are CRUCIAL to your homework assignment.**\n",
    "* Follow PEP 8 standards. Use the `pep8` tool in Jupyter Notebook to ensure proper formatting (however, note that it does not catch everything!).\n",
    "* Keep comments concise and strategic. Don't comment every line!\n",
    "* Organize your code in a way that makes it easy to follow. \n",
    "* Write your code so that it can be run on any operating system. This means that:\n",
    "   1. the data should be downloaded in the notebook to ensure it's reproducible.\n",
    "   2. all paths should be created dynamically using the os package to ensure that they work across operating systems. \n",
    "* Check for spelling errors in your text and code comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Richard Udell\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Colored Bar](colored-bar.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: single\n",
    "category: courses\n",
    "title: \"Learn to Create Efficient Data Workflows in Python\"\n",
    "permalink: /courses/earth-analytics-python/create-efficient-data-workflows/\n",
    "modified: '{:%Y-%m-%d}'.format(datetime.now())\n",
    "week-landing: 10\n",
    "week: 10\n",
    "sidebar:\n",
    "  nav:\n",
    "comments: false\n",
    "author_profile: false\n",
    "course: \"earth-analytics-python\"\n",
    "module-type: 'session'\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18ae14340de785a228745691ca76254e",
     "grade": false,
     "grade_id": "cell-a4b428e82167b1e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "{% include toc title=\"This Week\" icon=\"file-text\" %}\n",
    "\n",
    "<div class=\"notice--info\" markdown=\"1\">\n",
    "\n",
    "## <i class=\"fa fa-ship\" aria-hidden=\"true\"></i> Welcome to Week {{ page.week }}!\n",
    "\n",
    "Welcome to week {{ page.week }} of Earth Analytics! This week you will learn how to automate a workflow using `Python`. You will design and implement your own workflow in `Python` that builds on the skills that you have learned in this course, such as functions and loops. You will also learn how to programmatically build paths to directories and files as well as parse strings to extract information from file and directory names.  \n",
    "\n",
    "{% include/data_subsets/course_earth_analytics/_data-landsat-automation.md %}\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "## Automate a Workflow in Python\n",
    "\n",
    "For this week’s assignment, you will generate a plot of the normalized difference vegetation index (NDVI) for two different locations in the United States to begin to understand how the growing seasons vary in each site:\n",
    "\n",
    "1. <a href=\"https://www.neonscience.org/field-sites/field-sites-map/SJER\" target=\"_blank\">San Joaquin Experimental Range (SJER) in Southern California, United States</a>\n",
    "2. <a href=\"https://www.neonscience.org/field-sites/field-sites-map/HARV\" target=\"_blank\">Harvard Forest (HARV) in the Eastern United States</a> \n",
    "\n",
    "From this plot, you will be able to compare the seasonal vegetation patterns of the two locations. This comparison would be useful if you were planning NEON’s upcoming flight season in both locations and wanted to ensure that you flew the area when the vegetation was the most green! If could also be useful if you wanted to track green-up as it happened over time in both sites to see if there were changes happening. \n",
    "\n",
    "As a bonus, you will also create a stacked NDVI output data product to share with your colleagues. You are doing all of the work to clean and process the data. It would be nice if you could share a data product output to save others the hassle. \n",
    "\n",
    "## Design A Workflow \n",
    "\n",
    "Your goal this week is to calculate the mean NDVI value for each Landsat 8 scene captured for a NEON site over a year. You have the following data to do accomplish this goal:\n",
    "\n",
    "1. One year worth of Landsat 8 data for each site: Remember that for each landsat scene, you have a series of geotiff files representing bands and qa (quality assurance) layers in your data.\n",
    "2. A site boundary “clip file” for each site: This is a shapefile representing the boundary of each NEON site. You will want to clip your landsat data to this boundary.\n",
    "\n",
    "Before writing `Python` code, write pseudocode for your implementation. Pseudo-coding means that you will write out all of the steps that you need to perform. Then, you will identify areas where tasks are repeated that could benefit from a function, areas where loops might be appropriate, etc.  \n",
    "\n",
    "\n",
    "## Homework for this Week\n",
    "\n",
    "Your homework for this week is provided in the assignment dropbox on Canvas.\n",
    "\n",
    "\n",
    "## Homework Plots\n",
    "\n",
    "The plots below are examples of what your output plots will look like with and without dealing with clouds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5735bd6b424c29aa8c0f892d929d4445",
     "grade": false,
     "grade_id": "cell-75e446b2a4a1bfaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograding imports - do not modify this cell\n",
    "\n",
    "import matplotcheck.autograde as ag\n",
    "import matplotcheck.notebook as nb\n",
    "import matplotcheck.timeseries as ts\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b49c7be0a9154f60de2a7f032da3f6bf",
     "grade": true,
     "grade_id": "cell-2d2391d865730d53",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide",
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches as mpatches, colors\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.plot import plotting_extent\n",
    "from rasterio.mask import mask\n",
    "import earthpy as et\n",
    "import earthpy.spatial as es\n",
    "import earthpy.plot as ep\n",
    "import earthpy.mask as em\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Imports for box function?\n",
    "\n",
    "\n",
    "# Get data and set working directory\n",
    "data = et.data.get_data('ndvi-automation')\n",
    "os.chdir(os.path.join(et.io.HOME, 'earth-analytics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "945980a0c682bd079c4f3e91e3415cdc",
     "grade": false,
     "grade_id": "cell-1e463322f3d6c1a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Import SJER site boundary (.shp)\n",
    "sjer_base_path = os.path.join('data', 'ndvi-automation', 'sites', 'SJER')\n",
    "sjer_site_boundary_path = os.path.join(\n",
    "    sjer_base_path, 'vector', 'SJER-crop.shp')\n",
    "sjer_site_boundary = gpd.read_file(sjer_site_boundary_path, masked=True)\n",
    "\n",
    "# Import HARV site boundary (.shp)\n",
    "harv_base_path = os.path.join('data', 'ndvi-automation', 'sites', 'HARV')\n",
    "harv_site_boundary_path = os.path.join(\n",
    "    harv_base_path, 'vector', 'HARV-crop.shp')\n",
    "harv_site_boundary = gpd.read_file(harv_site_boundary_path, masked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best attempt at automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of directories via glob\n",
    "path = os.path.join('data','ndvi-automation','*','*','landsat-crop','*')\n",
    "all_dirs = glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automation baby!!!\n",
    "all_ndvi = []\n",
    "\n",
    "for directory in all_dirs:\n",
    "    # Extract the date of the directory \n",
    "    date = os.path.basename(os.path.normpath(directory))\n",
    "    date = date[10:18]\n",
    "    all_ndvi.append(date)\n",
    "    \n",
    "    # Extract qa and open via context manager\n",
    "    qa_glob = glob(os.path.join(directory + '/*pixel_qa*.tif'))\n",
    "    with rio.open(qa_glob[0]) as qa_src:\n",
    "        qa = qa_src.read(1)\n",
    "    \n",
    "    # Exctract all_bands(sorted) via glob from each directory\n",
    "    all_bands_glob = glob(os.path.join(directory + '/*band*.tif'))\n",
    "    all_bands_glob.sort()\n",
    "    \n",
    "    # Run stack function\n",
    "    output_path = os.path.join('data','ndvi-automation','outputs','file.tif')\n",
    "    land_stack, land_meta = es.stack(all_bands_glob, output_path)\n",
    "    \n",
    "     \n",
    "    # Open stack ouput .tif and read all_bands via context manager\n",
    "    with rio.open(output_path) as bands_src:\n",
    "        all_bands = bands_src.read() # Missing cloud management (come back to this)\n",
    "                               # Also missing: clipping to the site boundary (come back to this)\n",
    "    \n",
    "#     # Mask for cloud cover\n",
    "#     high_cloud_confidence = em.pixel_flags[\"pixel_qa\"][\"L8\"][\"High Cloud Confidence\"]\n",
    "#     all_bands_free = em.mask_pixels(arr=all_bands, mask_arr=qa, vals=high_cloud_confidence)\n",
    "    \n",
    "    \n",
    "    # Calculate NDVI\n",
    "    ndvi = es.normalized_diff(all_bands[4], all_bands[3]) \n",
    "    \n",
    "    # Save ndvi's to this list\n",
    "    all_ndvi.append(ndvi)\n",
    "    \n",
    "    \n",
    "len(all_ndvi) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with one scene for one site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of directories via glob\n",
    "path = os.path.join('data','ndvi-automation','*','SJER','landsat-crop','*')\n",
    "SJER_dirs = glob(path)\n",
    "\n",
    "path = os.path.join('data','ndvi-automation','*','HARV','landsat-crop','*')\n",
    "HARV_dirs = glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the masking variable we will use\n",
    "# Create a list of values that you want to set as \"mask\" in the pixel qa layer   \n",
    "high_cloud_confidence = em.pixel_flags[\"pixel_qa\"][\"L8\"][\"High Cloud Confidence\"]\n",
    "cloud = em.pixel_flags[\"pixel_qa\"][\"L8\"][\"Cloud\"]\n",
    "cloud_shadow = em.pixel_flags[\"pixel_qa\"][\"L8\"][\"Cloud Shadow\"]\n",
    "\n",
    "all_masked_values = cloud_shadow + cloud + high_cloud_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/ndvi-automation/sites/HARV/landsat-crop/LC080130302017041801T1-SC20181023152618'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'data/ndvi-automation/sites/HARV/landsat-crop/LC080130302017041801T1-SC20181023152618'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HARV_dirs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the dictionary that we will populate\n",
    "dict_to_df = {}\n",
    "\n",
    "\n",
    "def calc_ndvi_mean(list_of_dirs, site_boundary_shp):  # Still needs docstring...help-text\n",
    "    \"\"\"Loop through given list of Landsat8 directories to clip each to the given site\n",
    "    boundary, extract the date and site name, and calculate the mean NDVI for each scene.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_dirs : list\n",
    "        List of Landsat8 scene directories in Landsat Collection 1\n",
    "        Level-1 product identifier format: \n",
    "        LXSS_LLLL_PPPRRR_YYYYMMDD_yyyymmdd_CC_TX.\n",
    "\n",
    "    site_boundary_shp : geopandas.GeoDataFrame\n",
    "        .shp file imported into a GeoDataFrame.  Site boundary\n",
    "        must be entirely within the Landsat8 scene.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    dict_to_df : dictionary\n",
    "        Dictionary with the scene date as the key and the mean NDVI\n",
    "        and the site name as the value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    for directory in list_of_dirs:\n",
    "\n",
    "        # Extract the site name\n",
    "        site_name = directory[27:31]\n",
    "\n",
    "        # Extract the date of the directory\n",
    "        date = os.path.basename(os.path.normpath(directory))\n",
    "        date = date[10:18]\n",
    "\n",
    "        # Extract qa and open via context manager\n",
    "        qa_glob = glob(os.path.join(directory + '/*pixel_qa*.tif'))\n",
    "        with rio.open(qa_glob[0]) as qa_src:\n",
    "            qa, qa_meta = es.crop_image(qa_src, site_boundary_shp)\n",
    "            #qa = qa_src.read(1)\n",
    "            qa_extent = plotting_extent(qa_src)  # Is this code necessary?\n",
    "\n",
    "        # Exctract all_bands(sorted) via glob from the directory\n",
    "        all_bands_glob = glob(os.path.join(directory + '/*band*.tif'))\n",
    "        all_bands_glob.sort()\n",
    "\n",
    "        # Run stack function\n",
    "        output_path = os.path.join(\n",
    "            'data', 'ndvi-automation', 'outputs', 'all_bands.tif')\n",
    "        land_stack, land_meta = es.stack(all_bands_glob, output_path)\n",
    "\n",
    "        # Open stack ouput .tif and read all_bands via context manager\n",
    "        with rio.open(output_path) as bands_src:\n",
    "            all_bands, all_bands_meta = es.crop_image(\n",
    "                bands_src, site_boundary_shp)\n",
    "\n",
    "        # Prepare masked values variable to only include mask values in qa layer\n",
    "        active_cloud_codes = []\n",
    "\n",
    "        for cloud_code in all_masked_values:\n",
    "            if np.any(cloud_code == qa):\n",
    "                active_cloud_codes.append(cloud_code)\n",
    "\n",
    "        # Eliminates duplicates\n",
    "        active_cloud_codes = list(set(active_cloud_codes))\n",
    "\n",
    "        # Masking\n",
    "        # Mask all_bands with qa layer if necessary and calculate NDVI\n",
    "        if len(active_cloud_codes) != 0:\n",
    "            all_bands_masked = em.mask_pixels(arr=all_bands,\n",
    "                                              mask_arr=qa,\n",
    "                                              vals=active_cloud_codes)\n",
    "\n",
    "            ndvi = es.normalized_diff(all_bands_masked[4], all_bands_masked[3])\n",
    "        else:\n",
    "            ndvi = ndvi = es.normalized_diff(all_bands[4], all_bands[3])\n",
    "\n",
    "        mean_ndvi = ndvi.mean()\n",
    "\n",
    "        # Now let's get that dictionary going\n",
    "        # Add date as key and add value pair as ndvi.mean() and site_name\n",
    "        dict_to_df.update({date: (mean_ndvi, site_name)})\n",
    "        \n",
    "    return dict_to_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20170904': (masked, 'SJER'),\n",
       " '20170819': (0.32750823563090536, 'SJER'),\n",
       " '20171022': (0.31722907031043807, 'SJER'),\n",
       " '20171107': (0.3137839639712589, 'SJER'),\n",
       " '20170702': (0.3346638151749849, 'SJER'),\n",
       " '20170107': (masked, 'SJER'),\n",
       " '20170920': (0.33103548523629656, 'SJER'),\n",
       " '20170515': (0.441691751593773, 'SJER'),\n",
       " '20170429': (0.6104344457442622, 'SJER'),\n",
       " '20171123': (0.32494695680446906, 'SJER'),\n",
       " '20170616': (0.3587106397670503, 'SJER'),\n",
       " '20170312': (0.663938167189907, 'SJER'),\n",
       " '20170208': (masked, 'SJER'),\n",
       " '20170803': (masked, 'SJER'),\n",
       " '20170123': (masked, 'SJER'),\n",
       " '20171209': (0.35229998591810596, 'SJER'),\n",
       " '20171225': (0.27246451352603895, 'SJER'),\n",
       " '20170531': (masked, 'SJER'),\n",
       " '20171006': (0.3055060260766697, 'SJER'),\n",
       " '20170224': (0.663272893252757, 'SJER'),\n",
       " '20170413': (masked, 'SJER'),\n",
       " '20170328': (0.7029135204276337, 'SJER'),\n",
       " '20170718': (0.31988653659400057, 'SJER'),\n",
       " '20170418': (0.5418116737594111, 'HARV'),\n",
       " '20170317': (0.28402842011750823, 'HARV'),\n",
       " '20170707': (masked, 'HARV'),\n",
       " '20170213': (masked, 'HARV'),\n",
       " '20170824': (0.8637327011244158, 'HARV'),\n",
       " '20170808': (masked, 'HARV'),\n",
       " '20170301': (masked, 'HARV'),\n",
       " '20171128': (0.6182396540199216, 'HARV'),\n",
       " '20171027': (0.6881599915679869, 'HARV'),\n",
       " '20170504': (0.5698916537181732, 'HARV'),\n",
       " '20170925': (0.8400608927970141, 'HARV'),\n",
       " '20170112': (masked, 'HARV'),\n",
       " '20171214': (0.5262870513839859, 'HARV'),\n",
       " '20170621': (0.8816778750410028, 'HARV'),\n",
       " '20171011': (0.6532516220204823, 'HARV'),\n",
       " '20171112': (0.6132403402186392, 'HARV'),\n",
       " '20170520': (0.8110552930547179, 'HARV'),\n",
       " '20170128': (masked, 'HARV'),\n",
       " '20171230': (masked, 'HARV'),\n",
       " '20170723': (0.8200739748408408, 'HARV'),\n",
       " '20170605': (masked, 'HARV'),\n",
       " '20170402': (0.25323045664557053, 'HARV'),\n",
       " '20170909': (0.859943359957907, 'HARV')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_ndvi_mean(SJER_dirs, sjer_site_boundary)\n",
    "calc_ndvi_mean(HARV_dirs, harv_site_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ndvi_2017_df = pd.DataFrame.from_dict(\n",
    "    dict_to_df, orient='index', columns=['mean_NDVI', 'site_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is to clean up and plot the dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep.plot_bands(all_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep.plot_bands(unmasked_ndvi, cmap=\"Reds\", scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da76738e8661c744a09348fb70336d7f",
     "grade": true,
     "grade_id": "cell-d86b87f27caf0d5e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test your dataframe! For each test, rename the dataframe comparison to check against your dataframe.\n",
    "\n",
    "# For example, in test one, the date test, change \"student_dataframe['date_column']\" to match\n",
    "# your dataframe's name and date column. If your dataframe was called ndvi_data and the dates column was named\n",
    "# date_time, than the lines should look like `assert date_series.equals(ndvi_data['date_time'].sort_values())`\n",
    "\n",
    "# These tests are not graded so if they fail but your graph is correct\n",
    "# you won't be docked any points! Leave the sort values statement, only replace the dataframe and column names.\n",
    "\n",
    "# This is for data that hasn't been masked yet, it should be a half way sanity check.\n",
    "\n",
    "try:\n",
    "    assert site_series.equals(ndvi_ts['site'].sort_values())\n",
    "    print('Your site names appear to be accurate!')\n",
    "except AssertionError:\n",
    "    print('There seems to be an issue with your site names, check that column to make sure the values are correct.')\n",
    "try:\n",
    "    assert date_series.equals(ndvi_ts['date'].sort_values())\n",
    "    print('Your date values appear to be accurate!')\n",
    "except AssertionError:\n",
    "    print('There seems to be an issue with your date values, check that column to make sure the values are correct.')\n",
    "try:\n",
    "    assert ndvi_series.equals(ndvi_ts['ndvi'].sort_values())\n",
    "    print('Your ndvi values appear to be accurate!')\n",
    "except AssertionError:\n",
    "    print('There seems to be an issue with your ndvi values, check that column to make sure the values are correct.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "caption": "While there can exist month-to-month variability in NDVI values due to natural vegetation changes, the NDVI values for some months in this plot are the result of heavy cloud cover over the site.",
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5b2dbe48cc1a84b70d34baea9fa4466",
     "grade": false,
     "grade_id": "cell-094ad248a67beb76",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d4a81a1a4bb7362af50e61fded3af9",
     "grade": false,
     "grade_id": "cell-f71de353817fae9e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35565ac53b9fa1042cd62cb54142c5ee",
     "grade": false,
     "grade_id": "cell-0dbb343e769a09db",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "caption": "Plot showing NDVI for each time period at both NEON Sites. In this example the cloudy pixels were removed using the pixel_qa cloud mask. Notice that this makes a significant different in the output values. Why do you think this difference is so significant?",
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fd1bdad8acb93d3924cb33ebbea4b08",
     "grade": false,
     "grade_id": "cell-10287141af53f723",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "### DO NOT REMOVE LINE BELOW ###\n",
    "final_masked_solution = nb.convert_axes(plt, which_axes=\"current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a4afc1e80eed2c708aa384e730767d1",
     "grade": true,
     "grade_id": "cell-963612e7cefa2a5e",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
